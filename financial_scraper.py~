# financial_scraper.py (v10 - Agregujący dane ze wszystkich źródeł)

import requests
from bs4 import BeautifulSoup
import json
import os
import config
import re
from typing import Optional, Dict

STANDARD_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
AJAX_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'X-Requested-With': 'XMLHttpRequest'
}


def get_company_url_name(ticker: str) -> Optional[str]:
    search_url = "https://www.biznesradar.pl/gielda/akcje_gpw"
    try:
        response = requests.get(search_url, headers=STANDARD_HEADERS, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')
        ticker_link = soup.find('a', string=re.compile(f"^{ticker}$", re.IGNORECASE))
        if not ticker_link: return None
        href = ticker_link.get('href')
        company_url_name = href.split('/')[-1]
        print(f"Znaleziono mapowanie dla {ticker} -> {company_url_name}")
        return company_url_name
    except requests.exceptions.RequestException as e:
        print(f"Błąd podczas wyszukiwania URL dla {ticker}: {e}")
        return None


def get_financial_data(ticker: str) -> Optional[Dict]:
    company_url_name = get_company_url_name(ticker)
    if not company_url_name: return None

    possible_api_paths = [
        "https://www.biznesradar.pl/raporty-finansowe-rachunek-zyskow-i-strat",
        "https://www.biznesradar.pl/raporty-finansowe-skonsolidowane",
        "https://www.biznesradar.pl/raporty-finansowe-jednostkowe"
    ]

    # Słownik, który będzie agregował dane ze wszystkich ścieżek
    financial_data: Dict[str, Dict] = {}

    for base_path in possible_api_paths:
        headers_api_url = f"{base_path}/{company_url_name},Q,0,0"
        body_api_url = f"{base_path}/{company_url_name},Q,1,0"

        try:
            headers_response = requests.get(headers_api_url, headers=AJAX_HEADERS, timeout=10)
            body_response = requests.get(body_api_url, headers=AJAX_HEADERS, timeout=10)

            if headers_response.status_code == 200 and body_response.status_code == 200:
                header_soup = BeautifulSoup(headers_response.text, 'lxml')
                body_soup = BeautifulSoup(body_response.text, 'lxml')

                header_rows = header_soup.find_all('tr')
                body_rows = body_soup.find_all('tr')

                if not (header_rows and body_rows): continue

                print(f"Przetwarzam dane ze ścieżki: {base_path.split('/')[-1]}")
                all_rows = header_rows + body_rows

                headers = []
                header_row = all_rows[0]
                for th in header_row.find_all('th')[1:]:
                    headers.append(th.get_text(strip=True).replace(" ", ""))

                data_rows = all_rows[1:]

                target_rows_map = {
                    "Przychody ze sprzedaży": "revenue",
                    "Zysk operacyjny (EBIT)": "operating_profit"
                }

                for row in data_rows:
                    row_title_element = row.find('span', class_='report-name')
                    if not row_title_element: continue

                    row_title = row_title_element.get_text(strip=True)
                    if row_title in target_rows_map:
                        dict_key = target_rows_map[row_title]

                        # Jeśli klucz (np. 'revenue') już istnieje, nie nadpisujemy go
                        if dict_key in financial_data: continue

                        values = []
                        for td in row.find_all('td')[1:]:
                            value_str = td.get_text(strip=True)
                            try:
                                values.append(int(value_str.replace(' ', '')) * 1000)
                            except (ValueError, TypeError):
                                values.append(None)

                        financial_data[dict_key] = dict(zip(headers, values))

        except requests.exceptions.RequestException:
            continue

    # Zwracamy zagregowane dane, nawet jeśli są niekompletne
    return financial_data


def save_financials_to_json(ticker: str, data: dict):
    financials_path = os.path.join(config.data_path, "financials")
    if not os.path.exists(financials_path):
        os.makedirs(financials_path)
    file_path = os.path.join(financials_path, f"{ticker}.json")
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"Dane finansowe dla {ticker} zapisane w {file_path}")


if __name__ == "__main__":
    test_tickers = ["CDR", "PEP", "PKO", "KGHM", "ALE"]

    for ticker in test_tickers:
        print(f"\n--- Przetwarzanie spółki: {ticker} ---")
        data = get_financial_data(ticker)

        # Sprawdzamy, czy udało się zebrać WSZYSTKIE potrzebne dane
        if data and "revenue" in data and "operating_profit" in data:
            print(f"Pobrano kompletne dane dla {ticker}.")
            save_financials_to_json(ticker, data)
        else:
            print(
                f"Nie udało się pobrać kompletnych danych dla {ticker}. Zebrano: {list(data.keys()) if data else 'nic'}")